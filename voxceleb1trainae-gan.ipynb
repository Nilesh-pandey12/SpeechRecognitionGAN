{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e9ccf9",
   "metadata": {
    "id": "P72b5B0SutmA",
    "papermill": {
     "duration": 0.008033,
     "end_time": "2023-08-12T16:52:14.498201",
     "exception": false,
     "start_time": "2023-08-12T16:52:14.490168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Importation of the important librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda6d7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:52:14.515211Z",
     "iopub.status.busy": "2023-08-12T16:52:14.514350Z",
     "iopub.status.idle": "2023-08-12T16:52:18.290680Z",
     "shell.execute_reply": "2023-08-12T16:52:18.289399Z"
    },
    "id": "Q6uYK0S1uez0",
    "papermill": {
     "duration": 3.788027,
     "end_time": "2023-08-12T16:52:18.293623",
     "exception": false,
     "start_time": "2023-08-12T16:52:14.505596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7a97d",
   "metadata": {
    "id": "cqnCztZhvBKt",
    "papermill": {
     "duration": 0.00698,
     "end_time": "2023-08-12T16:52:18.308151",
     "exception": false,
     "start_time": "2023-08-12T16:52:18.301171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0d94b",
   "metadata": {
    "id": "qTY6QUyXnPzX",
    "papermill": {
     "duration": 0.007228,
     "end_time": "2023-08-12T16:52:18.322549",
     "exception": false,
     "start_time": "2023-08-12T16:52:18.315321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Accessing the .wav audio files and converting them to .npy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175164f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T16:52:18.339350Z",
     "iopub.status.busy": "2023-08-12T16:52:18.338714Z",
     "iopub.status.idle": "2023-08-12T18:40:49.392213Z",
     "shell.execute_reply": "2023-08-12T18:40:49.383268Z"
    },
    "id": "lMvgPT1EbNao",
    "papermill": {
     "duration": 6511.091409,
     "end_time": "2023-08-12T18:40:49.421398",
     "exception": false,
     "start_time": "2023-08-12T16:52:18.329989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(file_path, target_sampling_rate=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sampling_rate)\n",
    "    return audio, sr\n",
    "\n",
    "def extract_mfcc(audio, sr, n_mfcc=13):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfccs\n",
    "\n",
    "os.makedirs('/kaggle/working/Voxcelebfiles/MFCCFiles',exist_ok=True)\n",
    "# Replace 'your_dataset_directory' with the path to the LibriSpeech dataset\n",
    "dataset_directory = '/kaggle/input/voxceleb1train/wav'\n",
    "for dirname, _, filenames in os.walk(dataset_directory):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.wav'):\n",
    "                # print(audio_file_name, \"hii3\")\n",
    "\n",
    "                # Load and process audio\n",
    "                audio, sr = load_audio(os.path.join(dirname, filename))\n",
    "                mfccs = extract_mfcc(audio, sr)\n",
    "\n",
    "                np.save(f'/kaggle/working/Voxcelebfiles/MFCCFiles/{filename}_mfcc.npy', mfccs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d57a3",
   "metadata": {
    "id": "5Bs9Rr01nZvZ",
    "papermill": {
     "duration": 0.013769,
     "end_time": "2023-08-12T18:40:49.474411",
     "exception": false,
     "start_time": "2023-08-12T18:40:49.460642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Transforming the dataset using dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4684c436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T18:40:49.504604Z",
     "iopub.status.busy": "2023-08-12T18:40:49.497262Z",
     "iopub.status.idle": "2023-08-12T18:40:50.036129Z",
     "shell.execute_reply": "2023-08-12T18:40:50.034348Z"
    },
    "id": "vqzhVmtyP3jj",
    "papermill": {
     "duration": 0.560287,
     "end_time": "2023-08-12T18:40:50.042777",
     "exception": true,
     "start_time": "2023-08-12T18:40:49.482490",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m176\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMFCCDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mMFCCDataset.__init__\u001b[0;34m(self, dataset_directory, max_seq_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Load MFCC data from the .npy file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         mfcc_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43maudio_path\u001b[49m, audio_file_name))\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Pad the MFCC sequence to a fixed length\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         mfcc_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_sequence(mfcc_data, max_seq_len)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Custom dataset class to load MFCC data from .npy files\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, dataset_directory, max_seq_len):\n",
    "        self.data = []\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Loop through speaker directories\n",
    "        for dirname, _, filenames in os.walk(dataset_directory):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.npy'):\n",
    "                    # Load MFCC data from the .npy file\n",
    "                    mfcc_data = np.load(os.path.join(audio_path, audio_file_name))\n",
    "                    # Pad the MFCC sequence to a fixed length\n",
    "                    mfcc_data = self.pad_sequence(mfcc_data, max_seq_len)\n",
    "                    self.data.append(mfcc_data)\n",
    "\n",
    "        # Convert the list to a tensor after all data is collected\n",
    "        self.data = torch.tensor(self.data).float()\n",
    "\n",
    "    def pad_sequence(self, sequence, max_seq_len):\n",
    "        # Pad or truncate the sequence to max_seq_len\n",
    "        if len(sequence[0]) < max_seq_len:\n",
    "            sequence = np.pad(sequence, ((0, 0), (0, max_seq_len - len(sequence[0]))), mode='constant')\n",
    "        elif len(sequence[0]) > max_seq_len:\n",
    "            sequence = sequence[:, :max_seq_len]\n",
    "        return sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Set the path to the directory containing the .npy files\n",
    "npy_directory = '/kaggle/working/Voxcelebfiles/MFCCFiles'\n",
    "\n",
    "# Set the maximum sequence length for padding\n",
    "max_seq_len = 176\n",
    "\n",
    "# Load the dataset\n",
    "dataset = MFCCDataset(npy_directory, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f6ecd",
   "metadata": {
    "id": "wq8TjA-vuvI7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Auto Encoder Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0738e8",
   "metadata": {
    "id": "LPntFyCuu0xL",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Autoencoder model definition\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185c870",
   "metadata": {
    "id": "_7DD-6zzu-cs",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_dim = 13  # Number of MFCC coefficients\n",
    "latent_dim = 8\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize autoencoder model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder(input_dim, latent_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb8c68",
   "metadata": {
    "id": "kdXRJmi1YXra",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### **Saving the .npy files generated by decoder and the latent space also.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4da58",
   "metadata": {
    "id": "RXDeLY2J5BBR",
    "outputId": "d7f2ba85-0d06-47c7-dcee-39b47c43f084",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# Directory to save generated samples\n",
    "sample_dir = '/kaggle/working/Voxcelebfiles/AE_generated_samples'\n",
    "sample_dir1='/kaggle/working/Voxcelebfiles/LatentSpaceFile'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# List to store latent space representations\n",
    "latent_space_representations = []\n",
    "\n",
    "losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # Initialize loss for the current epoch\n",
    "    for mfcc_batch in dataset:\n",
    "        mfcc_batch = mfcc_batch.to(device)\n",
    "\n",
    "        # Reshape MFCC batch to match the input dimension\n",
    "        mfcc_batch = mfcc_batch.view(-1, input_dim)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(mfcc_batch)\n",
    "\n",
    "        # Compute loss and backpropagation\n",
    "        loss = criterion(outputs, mfcc_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / len(dataset)\n",
    "\n",
    "    # Append the average loss to the list for plotting\n",
    "    losses.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Generate and save samples\n",
    "    with torch.no_grad():\n",
    "        sample_noise = torch.randn(5, latent_dim).to(device)\n",
    "        generated_samples = model.decoder(sample_noise)\n",
    "\n",
    "        #extracting latent space\n",
    "        encoded_samples = model.encoder(mfcc_batch)\n",
    "        latent_space_representations.append(encoded_samples.detach().cpu().numpy())\n",
    "\n",
    "        # Reshape generated samples back to MFCC format\n",
    "        generated_samples = generated_samples.view(-1, input_dim)\n",
    "\n",
    "        # Save generated samples as .npy files\n",
    "        for i, sample in enumerate(generated_samples):\n",
    "            sample_file = os.path.join(sample_dir, f'epoch_{epoch+1}_sample_{i}.npy')\n",
    "            np.save(sample_file, sample.cpu().numpy())\n",
    "\n",
    "\n",
    "# Convert the list of latent space representations to a numpy array\n",
    "latent_space_representations = np.concatenate(latent_space_representations, axis=0)\n",
    "\n",
    "# Save latent space representations as a numpy array\n",
    "latent_space_file = os.path.join(sample_dir1, 'latent_space.npy')\n",
    "np.save(latent_space_file, latent_space_representations)\n",
    "\n",
    "\n",
    "# Save trained autoencoder model\n",
    "torch.save(model.state_dict(), '/kaggle/working/Voxcelebfiles/Model/autoencoder_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d305e",
   "metadata": {
    "id": "igEKEV5edOt-",
    "outputId": "1bbc1086-c0e7-40d5-b854-4ceb1f886810",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the loss function\n",
    "plt.plot(np.arange(1, num_epochs + 1), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a2dd0",
   "metadata": {
    "id": "nz1NH9ENXdLB",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Now Our Final GAN Model Will start.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623f91c",
   "metadata": {
    "id": "HosbJJK4XX1B",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **This File is for the latent space to train with generator and produce new samples, and discriminator with the Decoders output to descriminate with the generators output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3abe4c",
   "metadata": {
    "id": "jrlBIaykXpQ2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### **Loading the Mfcc data which was created by the AE Model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13ea9a",
   "metadata": {
    "id": "cyGSnUJ9XXlA",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom dataset class to load MFCC data from .npy files\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, dataset_directory, max_seq_len):\n",
    "        self.data = []\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        for audio_file_name in os.listdir(dataset_directory):\n",
    "          if audio_file_name.endswith('.npy'):\n",
    "            # Load MFCC data from the .npy file\n",
    "            mfcc_data = np.load(os.path.join(dataset_directory, audio_file_name))\n",
    "            self.data.append(mfcc_data)\n",
    "\n",
    "        # Convert the list to a tensor after all data is collected\n",
    "        self.data = torch.tensor(self.data).float()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Set the path to the directory containing the .npy files generated by Autoencoder Model\n",
    "npy_directory = '/kaggle/working/Voxcelebfiles/AE_generated_samples'\n",
    "\n",
    "# Set the maximum sequence length for padding\n",
    "max_seq_len = 176\n",
    "\n",
    "# Load the dataset\n",
    "dataset = MFCCDataset(npy_directory, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bce138",
   "metadata": {
    "id": "4xjNXN78Yhmo",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Loading the latent space data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcde7d",
   "metadata": {
    "id": "Y-R2V7RtYk8b",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the latent space file.\n",
    "latent_space_file = '/kaggle/working/Voxcelebfiles/LatentSpaceFile/latent_space.npy'\n",
    "latent_space_representations = np.load(latent_space_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175d623",
   "metadata": {
    "id": "Pfz9H1RgYpxX",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **The Generator and the Discriminator module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49357915",
   "metadata": {
    "id": "UGcZnr7MYrIJ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()  # Tanh activation to ensure generated values are within [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    # Define your discriminator architecture here\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define the discriminator architecture using nn.Sequential\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),        # Input layer: input_dim -> 512 units\n",
    "            nn.ReLU(True),                    # ReLU activation\n",
    "            nn.Linear(512, 256),              # Hidden layer: 512 units -> 256 units\n",
    "            nn.ReLU(True),                    # ReLU activation\n",
    "            nn.Linear(256, 1),                # Output layer: 256 units -> 1 unit (binary classification)\n",
    "            nn.Sigmoid()                      # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass for the discriminator\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3d9bc",
   "metadata": {
    "id": "q0-9_rgxYuSG",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **The Training loop and saving loop for saving the fresh data created by Gan(the final output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5fc7f",
   "metadata": {
    "id": "NZ0PFZKbY1yb",
    "outputId": "8e56b376-042f-433b-e3f2-0282179e16a1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def compute_generator_loss(generated_data, real_data):\n",
    "    # Compute Mean Squared Error (MSE) loss between generated and real data\n",
    "    mse_loss = nn.MSELoss()\n",
    "    loss = mse_loss(generated_data, real_data)\n",
    "    return loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Create DataLoaderdat\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize the generator\n",
    "# Define hyperparameters\n",
    "latent_dim = 8  # Dimensionality of the latent space\n",
    "output_dim = 13\n",
    "generator = Generator(latent_dim, output_dim)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Initialize discriminator\n",
    "discriminator = Discriminator(input_dim=13)  # Use the appropriate input dimension\n",
    "# Define optimizer for the discriminator\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\n",
    "num_epochs=20\n",
    "batch_size=64\n",
    "\n",
    "discriminator_losses = []\n",
    "generator_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for real_mfcc_batch in dataset:  # Your real data loader\n",
    "        real_mfcc_batch = real_mfcc_batch.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Zero the discriminator parameter gradients\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        # Train the discriminator with real data\n",
    "        real_labels = torch.ones(real_mfcc_batch.size(0), 1).to(device)  # Labels for real data\n",
    "        real_outputs = discriminator(real_mfcc_batch)\n",
    "\n",
    "        # Calculate the binary cross-entropy loss\n",
    "        real_labels_expanded = real_labels.expand_as(real_outputs)  # Expand the labels to match the shape of real_outputs\n",
    "        real_loss = criterion(real_outputs, real_labels_expanded)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # Generate fake data using the generator (decoder)\n",
    "        # Randomly sample latent space representations\n",
    "        batch_indices = np.random.randint(len(latent_space_representations), size=batch_size)\n",
    "        batch_latent_space = latent_space_representations[batch_indices]\n",
    "\n",
    "        batch_latent_space = torch.randn(real_mfcc_batch.size(0), latent_dim).to(device)  # Match batch size\n",
    "        generated_data = generator(batch_latent_space)\n",
    "\n",
    "        # Reshape the generated data to match the shape of real MFCC batch\n",
    "        generated_data = generated_data.view(-1, output_dim)  # Match the appropriate dimension\n",
    "\n",
    "\n",
    "        # Update the generator using a loss that encourages the generated data to match the real data distribution in the latent space\n",
    "        generator_optimizer.zero_grad()\n",
    "        generator_loss = compute_generator_loss(generated_data, real_mfcc_batch)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator with generated data\n",
    "        fake_labels = torch.zeros(generated_data.size(0), 1).to(device)  # Labels for fake data\n",
    "        fake_outputs = discriminator(generated_data.detach())  # Detach to prevent gradients from propagating to the generator\n",
    "        fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        # Update discriminator parameters\n",
    "        discriminator_optimizer.step()\n",
    "        # Append losses to their respective lists\n",
    "        discriminator_losses.append(real_loss.item() + fake_loss.item())\n",
    "        generator_losses.append(generator_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Discriminator Real Loss: {real_loss.item():.4f}, Discriminator Fake Loss: {fake_loss.item():.4f}')\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            # Generate a random noise vector\n",
    "            noise = torch.randn(1, latent_dim).to(device)\n",
    "\n",
    "            # Generate fake data using the Generator\n",
    "            fake_mfcc = generator(noise).squeeze().cpu().numpy()\n",
    "\n",
    "            # Calculate the number of frames based on the MFCC shape\n",
    "            n_frames = fake_mfcc.shape[0] // 13  # Assuming each frame has 13 coefficients\n",
    "\n",
    "            # Reshape the fake MFCCs to the correct shape\n",
    "            fake_mfcc = fake_mfcc.reshape(13, n_frames)\n",
    "\n",
    "            # Convert MFCCs back to audio\n",
    "            fake_audio = librosa.feature.inverse.mfcc_to_audio(fake_mfcc.T)\n",
    "\n",
    "            # Save the generated audio sample\n",
    "            generated_sample_path = os.path.join(\"/kaggle/working/Voxcelebfiles/GAN_generated_samples\", f'generated_sample_epoch_{epoch+1}_{i+1}.wav')\n",
    "            sf.write(generated_sample_path, fake_audio, 16000)\n",
    "            print(f'Saved generated sample: {generated_sample_path}')\n",
    "\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc391582",
   "metadata": {
    "id": "v4Pq0dYZY3Ii",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **Visualization of a sample data created by our GAN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501912d2",
   "metadata": {
    "id": "hPiKUifIY7DM",
    "outputId": "fae3d129-d769-41fe-d494-a6d5e2d4d1db",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "# Load a generated audio sample\n",
    "sample_path = \"/content/drive/MyDrive/Librispeechdatatset/GAN_generated_samples/generated_sample_epoch_1_1.wav\"\n",
    "audio, _ = librosa.load(sample_path, sr=16000)\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(audio, sr=16000)\n",
    "plt.title(\"Generated Audio Waveform\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "spec = librosa.amplitude_to_db(librosa.stft(audio), ref=np.max)\n",
    "librosa.display.specshow(spec, sr=16000, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Generated Audio Spectrogram\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91c640",
   "metadata": {
    "id": "N_vFoG0hlUHh",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(discriminator_losses, label='Discriminator Loss')\n",
    "plt.plot(generator_losses, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Generator and Discriminator Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6529.799048,
   "end_time": "2023-08-12T18:40:53.350697",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T16:52:03.551649",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
